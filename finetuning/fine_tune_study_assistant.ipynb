{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRCC2xELoS_E"
      },
      "source": [
        "# Fine-Tune a Generative AI Model for Study Assistant (Q&A)\n",
        "\n",
        "This notebook is adapted to work with the **study_assistant_data.jsonl** dataset.\n",
        "\n",
        "- **Data format**: `instruction`/`input`/`output`\n",
        "- **Task type**: question answering\n",
        "- **Prompt template**: Adapted for instruction-following Q&A format\n",
        "- **Data loading**: From local JSONL file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHRidwFFoS_F"
      },
      "source": [
        "---\n",
        "# Why Do We Need to Preprocess the Dataset?\n",
        "\n",
        "Preprocessing is **essential** for fine-tuning LLMs. Here's why:\n",
        "\n",
        "## 1. Format Compatibility\n",
        "LLMs don't understand raw JSON or text - they need **tokenized numerical sequences**.\n",
        "\n",
        "Your data looks like:\n",
        "```json\n",
        "{\"instruction\": \"You are My Learning Buddy...\", \"input\": \"What is the Pythagorean theorem?\", \"output\": \"The formula is a² + b² = c²...\"}\n",
        "```\n",
        "\n",
        "But the model needs:\n",
        "```\n",
        "input_ids: [425, 19, 8, 12901, 7, 9, 102, ...]\n",
        "labels: [37, 3, 31839, 32, 9, ...]\n",
        "```\n",
        "\n",
        "## 2. Tokenization Requirements\n",
        "- **Breaking text into tokens**: \"preprocessing\" → [\"pre\", \"process\", \"ing\"]\n",
        "- **Adding special tokens**: `<BOS>` (begin), `<EOS>` (end), `<PAD>` (padding)\n",
        "- **Uniform lengths**: All sequences padded/truncated to same length for efficient batching\n",
        "\n",
        "## 3. Prompt Engineering\n",
        "We wrap your data in a template so the model learns the expected format:\n",
        "```\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "{output}\n",
        "```\n",
        "\n",
        "## 4. Separating Input and Labels\n",
        "- **input_ids**: What the model sees (the question)\n",
        "- **labels**: What the model should generate (the answer)\n",
        "\n",
        "## 5. Memory Efficiency\n",
        "- GPUs process data in batches\n",
        "- All sequences must have the same length\n",
        "- Preprocessing creates uniform tensors for parallel computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTQjOW9foS_G"
      },
      "source": [
        "---\n",
        "# Table of Contents\n",
        "\n",
        "- [1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n",
        "  - [1.1 - Set up Kernel and Required Dependencies](#1.1)\n",
        "  - [1.2 - Load Dataset and LLM](#1.2)\n",
        "  - [1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n",
        "- [2 - Perform Full Fine-Tuning](#2)\n",
        "  - [2.1 - Preprocess the Q&A Dataset](#2.1)\n",
        "  - [2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n",
        "  - [2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n",
        "  - [2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n",
        "- [3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n",
        "  - [3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n",
        "  - [3.2 - Train PEFT Adapter](#3.2)\n",
        "  - [3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n",
        "  - [3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttBvVouXoS_G"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFCbROV4oS_G"
      },
      "source": [
        "<a name='1.1'></a>\n",
        "### 1.1 - Set up Kernel and Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tru8ocUtoS_G",
        "outputId": "a8f1c99c-c789-4f65-e0ed-28890b5e5b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.10.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.46.3)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from wheel) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -q torch torchdata\n",
        "!pip install -q transformers>=4.40.0 datasets evaluate rouge-score peft loralib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6ebPLi1oS_H"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8_lI5fKoS_H"
      },
      "source": [
        "<a name='1.2'></a>\n",
        "### 1.2 - Load Dataset and LLM\n",
        "\n",
        "We load the local `study_assistant_data.jsonl` file.\n",
        "\n",
        "The data has the format:\n",
        "- `instruction`: System instruction (\"You are My Learning Buddy...\")\n",
        "- `input`: The user's question\n",
        "- `output`: The expected answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW806dQfoS_H"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the study assistant dataset from JSONL file\n",
        "file_path = '/content/study_assistant_data.jsonl'\n",
        "dataset = load_dataset('json', data_files=file_path, split='train')\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"\\nDataset features: {dataset.features}\")\n",
        "print(f\"\\nFirst example:\")\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: 80% train, 20% temp\n",
        "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_test['train']\n",
        "temp_dataset = train_test['test']\n",
        "\n",
        "# Second split: 50% of temp for validation, 50% for test\n",
        "val_test = temp_dataset.train_test_split(test_size=0.5, seed=42)\n",
        "val_dataset = val_test['train']\n",
        "test_dataset = val_test['test']\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Create HuggingFace DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(f\"\\nDataset structure:\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "SOHlbdCms1Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXKpgmBToS_H"
      },
      "source": [
        "Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. FLAN-T5 is already instruction-tuned, making it a good choice for Q&A tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGy4e1spoS_I"
      },
      "outputs": [],
      "source": [
        "model_name = 'google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2XrUWqAoS_I"
      },
      "outputs": [],
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4-Hmvl2oS_I"
      },
      "source": [
        "<a name='1.3'></a>\n",
        "### 1.3 - Test the Model with Zero Shot Inferencing\n",
        "\n",
        "Let's test how the base model performs on your Q&A task WITHOUT fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpJpv6g_oS_I"
      },
      "outputs": [],
      "source": [
        "\n",
        "index = 30\n",
        "\n",
        "# Get sample from test set\n",
        "instruction = dataset['train'][index]['instruction']\n",
        "question = dataset['train'][index]['input']\n",
        "expected_answer = dataset['train'][index]['output']\n",
        "\n",
        "# Create prompt for Q&A\n",
        "prompt = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=500,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT QUESTION:\\n{question}')\n",
        "print(dash_line)\n",
        "print(f'EXPECTED ANSWER (first 500 chars):\\n{expected_answer[:500]}...')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5iJEGFkoS_I"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Perform Full Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2DjPnw0oS_I"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 - Preprocess the Q&A Dataset\n",
        "\n",
        "**This is where the magic happens!** We need to:\n",
        "\n",
        "1. Convert each Q&A pair into a prompt template\n",
        "2. Tokenize both the prompt (input) and the answer (labels)\n",
        "3. Pad/truncate to uniform lengths\n",
        "\n",
        "The tokenize function uses `instruction`, `input`, and `output` fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypGSBWBXoS_I"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenize_function(example):\n",
        "    \"\"\"\n",
        "    Preprocess the Q&A dataset:\n",
        "    - Creates prompt from instruction + input\n",
        "    - Tokenizes prompt as input_ids\n",
        "    - Tokenizes output as labels\n",
        "    \"\"\"\n",
        "    # Build the prompt template (Alpaca-style format)\n",
        "    start_prompt = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n'\n",
        "    middle_prompt = '\\n\\n### Input:\\n'\n",
        "    end_prompt = '\\n\\n### Response:\\n'\n",
        "\n",
        "    # Combine instruction and input into the prompt\n",
        "    prompts = [\n",
        "        start_prompt + inst + middle_prompt + inp + end_prompt\n",
        "        for inst, inp in zip(example[\"instruction\"], example[\"input\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenize the prompts (input to the model)\n",
        "    example['input_ids'] = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,  # Increased for longer instructions\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    # Tokenize the outputs/answers (what the model should generate)\n",
        "    example['labels'] = tokenizer(\n",
        "        example[\"output\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,  # Increased for longer answers\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# Apply tokenization to all splits\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the original text columns (no longer needed after tokenization)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['instruction', 'input', 'output'])\n",
        "\n",
        "print(\"Tokenization complete!\")\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO6B5LavoS_I"
      },
      "outputs": [],
      "source": [
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK5AedfUoS_I"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n",
        "\n",
        "Now we train the model using the HuggingFace `Trainer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk0KkaXAoS_J"
      },
      "outputs": [],
      "source": [
        "output_dir = f'./study-assistant-training-{str(int(time.time()))}'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=3,  # Increased epochs for small dataset\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    max_steps=100,  # Adjusted for small dataset\n",
        "    per_device_train_batch_size=4,  # Small batch size for small dataset\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjwmBTQwoS_J"
      },
      "outputs": [],
      "source": [
        "# Optional: Login to Weights & Biases for experiment tracking\n",
        "# Uncomment the following lines if you want to use wandb\n",
        "\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKk7p8RVoS_J"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_NGFC39oS_J"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./study-assistant-finetuned-checkpoint\")\n",
        "tokenizer.save_pretrained(\"./study-assistant-finetuned-checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFOhZ3BYoS_J"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model for inference\n",
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"./study-assistant-finetuned-checkpoint\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3yJcgX4oS_J"
      },
      "source": [
        "<a name='2.3'></a>\n",
        "### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n",
        "\n",
        "Let's compare the original model vs the fine-tuned model on the same question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPpHBMPXoS_J"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "# Pick a sample from test set\n",
        "index = 0\n",
        "instruction = dataset['test'][index]['instruction']\n",
        "question = dataset['test'][index]['input']\n",
        "expected_answer = dataset['test'][index]['output']\n",
        "\n",
        "# Create the prompt\n",
        "prompt = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Move models to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "original_model = original_model.to(device)\n",
        "instruct_model = instruct_model.to(device)\n",
        "\n",
        "# Tokenize input\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# Generation config\n",
        "gen_config = GenerationConfig(max_new_tokens=300, num_beams=1)\n",
        "\n",
        "# Generate outputs\n",
        "with torch.no_grad():\n",
        "    original_outputs = original_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "    instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "\n",
        "# Decode\n",
        "original_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "instruct_text = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print comparison\n",
        "dash_line = \"-\" * 80\n",
        "print(dash_line)\n",
        "print(f'QUESTION:\\n{question}')\n",
        "print(dash_line)\n",
        "print(f'EXPECTED ANSWER (first 500 chars):\\n{expected_answer[:500]}...')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL OUTPUT:\\n{original_text}')\n",
        "print(dash_line)\n",
        "print(f'FINE-TUNED MODEL OUTPUT:\\n{instruct_text}')\n",
        "print(dash_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40ThiStzoS_J"
      },
      "source": [
        "<a name='2.4'></a>\n",
        "### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "\n",
        "ROUGE metrics help measure how similar the generated text is to the expected output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV8C5pNQoS_J"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUqfX_SwoS_J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get all test samples\n",
        "test_instructions = dataset['test']['instruction']\n",
        "test_questions = dataset['test']['input']\n",
        "expected_answers = dataset['test']['output']\n",
        "\n",
        "# Initialize result lists\n",
        "original_model_answers = []\n",
        "instruct_model_answers = []\n",
        "\n",
        "gen_config = GenerationConfig(max_new_tokens=300, num_beams=1)\n",
        "\n",
        "# Loop through test samples\n",
        "for idx, (inst, question) in enumerate(zip(test_instructions, test_questions)):\n",
        "    prompt = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{inst}\n",
        "\n",
        "### Input:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Original model\n",
        "        orig_out = original_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "        orig_text = tokenizer.decode(orig_out[0], skip_special_tokens=True)\n",
        "        original_model_answers.append(orig_text)\n",
        "\n",
        "        # Fine-tuned model\n",
        "        inst_out = instruct_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "        inst_text = tokenizer.decode(inst_out[0], skip_special_tokens=True)\n",
        "        instruct_model_answers.append(inst_text)\n",
        "\n",
        "    print(f\"✅ Processed sample {idx+1}/{len(test_questions)}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'question': test_questions,\n",
        "    'expected_answer': expected_answers,\n",
        "    'original_model': original_model_answers,\n",
        "    'finetuned_model': instruct_model_answers\n",
        "})\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC-LLo-aoS_K"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE scores\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_answers,\n",
        "    references=expected_answers,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_answers,\n",
        "    references=expected_answers,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL ROUGE SCORES:')\n",
        "print(original_model_results)\n",
        "print('\\nFINE-TUNED MODEL ROUGE SCORES:')\n",
        "print(instruct_model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYYNbyOroS_K"
      },
      "outputs": [],
      "source": [
        "print(\"Absolute percentage improvement of FINE-TUNED MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(instruct_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKAFyz6EoS_K"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "PEFT allows us to fine-tune only a small number of parameters, saving compute resources while achieving good results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOheffGRoS_K"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY7NM_VHoS_K"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,  # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM  # FLAN-T5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yw2l4eOoS_K"
      },
      "outputs": [],
      "source": [
        "# Reload the original model for PEFT training\n",
        "original_model_for_peft = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(original_model_for_peft, lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcTrwwHxoS_K"
      },
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Train PEFT Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR2iazSwoS_K"
      },
      "outputs": [],
      "source": [
        "output_dir = f'./peft-study-assistant-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,  # Higher learning rate for PEFT\n",
        "    num_train_epochs=5,  # More epochs for small dataset\n",
        "    logging_steps=1,\n",
        "    max_steps=100\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWUvVyPXoS_K"
      },
      "outputs": [],
      "source": [
        "# Train PEFT model\n",
        "peft_trainer.train()\n",
        "\n",
        "# Save the PEFT model\n",
        "peft_model_path = \"./peft-study-assistant-checkpoint-local-l\"\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVbyfd9NoS_L"
      },
      "source": [
        "<a name='3.3'></a>\n",
        "### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YeFpPB4oS_L"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Load PEFT model for inference\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"google/flan-t5-base\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(\n",
        "    peft_model_base,\n",
        "    './peft-study-assistant-checkpoint-local-l/',\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    is_trainable=False\n",
        ")\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8tB8w6SoS_L"
      },
      "outputs": [],
      "source": [
        "# Compare all three models on a test sample\n",
        "index = 0\n",
        "instruction = dataset['test'][index]['instruction']\n",
        "question = dataset['test'][index]['input']\n",
        "expected_answer = dataset['test'][index]['output']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# Move PEFT model to device\n",
        "peft_model = peft_model.to(device)\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "gen_config = GenerationConfig(max_new_tokens=300, num_beams=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    original_outputs = original_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "    instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "    peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "\n",
        "original_text = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "instruct_text = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
        "peft_text = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "dash_line = \"-\" * 80\n",
        "print(dash_line)\n",
        "print(f'QUESTION:\\n{question}')\n",
        "print(dash_line)\n",
        "print(f'EXPECTED ANSWER (first 500 chars):\\n{expected_answer[:500]}...')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_text}')\n",
        "print(dash_line)\n",
        "print(f'FULL FINE-TUNED MODEL:\\n{instruct_text}')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL:\\n{peft_text}')\n",
        "print(dash_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHQcRTdoS_L"
      },
      "source": [
        "<a name='3.4'></a>\n",
        "### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP4ZKnvVoS_L"
      },
      "outputs": [],
      "source": [
        "# Generate PEFT model outputs for all test samples\n",
        "peft_model_answers = []\n",
        "\n",
        "for idx, (inst, question) in enumerate(zip(test_instructions, test_questions)):\n",
        "    prompt = f\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{inst}\n",
        "\n",
        "### Input:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        peft_out = peft_model.generate(input_ids=input_ids, generation_config=gen_config)\n",
        "        peft_text = tokenizer.decode(peft_out[0], skip_special_tokens=True)\n",
        "        peft_model_answers.append(peft_text)\n",
        "\n",
        "    print(f\"✅ Processed sample {idx+1}/{len(test_questions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjQIYEYloS_L"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE scores for PEFT model\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_answers,\n",
        "    references=expected_answers,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('\\nFULL FINE-TUNED MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('\\nPEFT MODEL:')\n",
        "print(peft_model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkEFFzpQoS_L"
      },
      "outputs": [],
      "source": [
        "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(peft_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK4seLd7oS_L"
      },
      "outputs": [],
      "source": [
        "print(\"Absolute percentage improvement of PEFT MODEL over FULL FINE-TUNED MODEL\")\n",
        "\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
        "for key, value in zip(peft_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AtkLY3ooS_L"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we:\n",
        "\n",
        "1. **Loaded your custom study assistant Q&A dataset** from a JSONL file\n",
        "2. **Preprocessed the data** by:\n",
        "   - Creating prompt templates with instruction/input/response format\n",
        "   - Tokenizing both inputs and labels\n",
        "   - Padding/truncating to uniform lengths\n",
        "3. **Performed full fine-tuning** on FLAN-T5\n",
        "4. **Performed PEFT/LoRA fine-tuning** with only ~0.3% of parameters\n",
        "5. **Evaluated both approaches** using ROUGE metrics\n",
        "\n",
        "### Key Takeaways:\n",
        "- **Preprocessing is essential** to convert text to tokenized tensors\n",
        "- **PEFT achieves comparable results** with much less compute\n",
        "- **With small datasets** like yours (50 samples), PEFT often works better due to less overfitting risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJpTSzDjoS_L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}